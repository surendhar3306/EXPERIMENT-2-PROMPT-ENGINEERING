# EXP-2-PROMPT-ENGINEERING-

## Aim: 
Comparative Analysis of different types of Prompting patterns and explain with Various Test Scenarios

Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
Analyze the quality, accuracy, and depth of the generated responses.


## Algorithm:
Select different prompting patterns (Zero-shot, Few-shot, Chain-of-thought, Instruction-based, Role-based, Unstructured).

Define test scenarios (Knowledge Q&A, Creative Writing, Problem Solving, Professional Task).

For each scenario, apply unstructured prompt and refined prompt.

Compare responses on quality, accuracy, and depth.

## Output
Unstructured Prompt: Responses are generic, often lacking depth and context.

Refined Prompt: Responses are structured, context-aware, and higher in accuracy.

Pattern Effect:

Instruction-based → Clear, professional responses.

Role-based → Contextual and audience-focused.

Few-shot → More creative and format-specific.

Chain-of-thought → Better reasoning in problem-solving.

## Result
Prompting pattern directly affects response quality.

Broad/unstructured prompts → useful for exploration but low precision.

Refined prompts → produce more accurate, detailed, and relevant answers.

Best-fit styles:

Knowledge tasks → Instruction-based + Role-based.

Creative tasks → Few-shot + Role-based.

Problem solving → Chain-of-thought.

Professional tasks → Instruction-based.
